
# Natural Language Processing
# Link to example
# https://colab.research.google.com/drive/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true&scrollTo=RACGE5Ypt5u9

# Natural Language Processing (or NLP for short) is a discipline in computing that deals with
# the communication between natural (human) languages and computer languages. A common example of NLP is
#  something like spellcheck or autocomplete. Essentially NLP is the field that focuses on how computers
# can understand and/or process natural/human languages.
#
# Recurrent Neural Networks
# In this tutorial we will introduce a new kind of neural network that is much more capable of
# processing sequential data such as text or characters called a recurrent neural network (RNN for short).
#
# We will learn how to use a reccurent neural network to do the following:
# Sentiment Analysis
# Character Generation
#
# RNN's are complex and come in many different forms so in this tutorial we wil focus on how they
# work and the kind of problems they are best suited for.

# one form of this is Bag of Words where all words used are tracked but the context is lost so this is a poor one
# Integer Encoding takes this a step further mantaining sentence structure with arrays but with lage vocabularys this fails
# Embedings-  is much better as it sets the words a layer of vectors where similar words are close and point togther

# we will use Word Embedings to make our model


# Recurrent Neural Networks (RNN's)
# Now that we've learned a little bit about how we can encode text it's time to dive into recurrent neural networks.
# Up until this point we have been using something called feed-forward neural networks. This simply means that all
# our data is fed forwards (all at once) from left to right through the network. This was fine for the problems we
#  considered before but won't work very well for processing text. After all, even we (humans) don't process text
# all at once. We read word by word from left to right and keep track of the current meaning of the sentence so
# we can understand the meaning of the next word. Well this is exaclty what a recurrent neural network is designed
# to do. When we say recurrent neural network all we really mean is a network that contains a loop. A RNN will process
# one word at a time while maintaining an internal memory of what it's already seen. This will allow it to treat words
# differently based on their order in a sentence and to slowly build an understanding of the entire input, one
#  word at a time.
#
# This is why we are treating our text data as a sequence! So that we can pass one word at a time to the RNN.
# we can do this as Long term RNN or simple RNN

# LSTM
# The layer we dicussed in depth above was called a simpleRNN. However, there does exist some other recurrent
#  layers (layers that contain a loop) that work much better than a simple RNN layer. The one we will talk
# about here is called LSTM (Long Short-Term Memory). This layer works very similarily to the simpleRNN layer
# but adds a way to access inputs from any timestep in the past. Whereas in our simple RNN layer input from
# previous timestamps gradually disappeared as we got further through the input. With a LSTM we have a long-term
# memory data structure storing all the previously seen inputs as well as when we saw them. This allows for us to
# access any previous value we want at any point in time. This adds to the complexity of our network and allows it to
# discover more useful relationships between inputs and when they appear.
#
# For the purpose of this course we will refrain from going any further into the math or details behind how these layers work.

#Movie Review Dataset for Setement Analysis

from keras.datasets import imdb
from keras.preprocessing import sequence
import keras
import tensorflow as tf
import os
import numpy as np


VOCAB_SIZE = 88584

MAXLEN = 250
BATCH_SIZE = 64

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)

# Lets look at one review
#print(train_data[1])

#next we do more preprocessing and reduce the data down to 250 or pad it out to 250 values
train_data = sequence.pad_sequences(train_data, MAXLEN)
test_data = sequence.pad_sequences(test_data, MAXLEN)
#print(train_data[1])

# Creating the Model
# Now it's time to create the model. We'll use a word embedding layer as the first layer in our model
# and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment.
# 32 stands for the output dimension of the vectors generated by the embedding layer. We can change
# this value if we'd like!
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE, 32),
    tf.keras.layers.LSTM(32),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

#print(model.summary())

model.compile(loss="binary_crossentropy",optimizer="rmsprop",metrics=['acc'])
history = model.fit(train_data, train_labels, epochs=2, validation_split=0.2)
results = model.evaluate(test_data, test_labels)
print(results)

# now we can use this model and put it to work on some data
# but first we have to pre process that data as well
word_index = imdb.get_word_index()

def encode_text(text):
  tokens = keras.preprocessing.text.text_to_word_sequence(text)
  tokens = [word_index[word] if word in word_index else 0 for word in tokens]
  return sequence.pad_sequences([tokens], MAXLEN)[0]

text = "that movie was just amazing, so amazing"
encoded = encode_text(text)
#print(encoded)

# while were at it lets make a decode function

reverse_word_index = {value: key for (key, value) in word_index.items()}

def decode_integers(integers):
    PAD = 0
    text = ""
    for num in integers:
        if num != PAD:
            text += reverse_word_index[num] + " "

    return text[:-1]

#print(decode_integers(encoded))


#now lets use our model to actualy make a prediction on some data
# now time to make a prediction

def predict(text):
  encoded_text = encode_text(text)
  pred = np.zeros((1,250))
  pred[0] = encoded_text
  result = model.predict(pred)
  print('1= very positive  0 = very negative')
  print(result[0])

positive_review = "That movie was! really loved it and would great watch it again because it was amazingly great"
predict(positive_review)

negative_review = "that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched"
predict(negative_review)


print("done")
